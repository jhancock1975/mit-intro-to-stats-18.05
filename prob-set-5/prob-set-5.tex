\documentclass[a5paper,11pt]{article}

%for coloring cell in a table
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor

\usepackage{amsmath}
\usepackage{amssymb}

% for proofs  environment
\usepackage{amsthm}

% for 3d plots
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepgfplotslibrary{patchplots}

\usepackage[backend=bibtex]{biblatex}
\bibliography{prob-set-5}

% for probability trees
\usepackage{tikz}
\usetikzlibrary{trees}

% for Venn diagrams
\usetikzlibrary{shapes,backgrounds}

% for plots
\usepackage{ pgfplots}
% inserted on suggestion in warning during compilation
\pgfplotsset{compat=1.9}

%for strikethrough text
\usepackage{soul}

%for R source code listing
\usepackage{listings}

%for block quotes
\usepackage{csquotes}

%for Theorems & Lemmas
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}

% For not indenting the first line of paragraphs:
\setlength{\parindent}{0pt}

% define the title
\author{John Hancock}
\title{Problem Set 5}

\begin{document}

% generates the title
\maketitle

% insert the table of contents
\tableofcontents

\section{References and License}
We are answering questions in the material from MIT OpenCourseWare
course 18.05, Introduction to Probability and Statistics.

In this document we are answering questions Orloff and Bloom ask in
\cite{probSet5}.

Please see the references section for detailed citation information.

The material for the course is licensed under the terms at
\url{http://ocw.mit.edu/terms}.

We use documentation in to write the \LaTeX source code for this document.

\section{Fit line to data}

In this section we answer questions about a random variable $Y$ drawn from
the random variable $Y_i \sim ax_i + b + \epsilon_i$, where $epslion_i$ is
a random variable with mean $0$ and variance $\sigma^2$. 

Orloff and Bloom grant us that the $\epsilon_i$ are independent.

\subsection{Likelihood function}

We derive the likelihood function $f\left(y_i \mid a,b,x_i, \sigma\right)$.

To derive $f$ we assume $x_i, y_i$, and $\sigma$ are known values. 

It is of paramount importance to note:
\begin{equation}
\epsilon_i \sim N\left(0, \sigma \right).
\end{equation}

We then look at the random variable:
\begin{equation}
Y_i = ax_i + b + \epsilon_i
\end{equation}


$\epsilon_i$ is a random variable that follows a normal distribution.  In
the context of this discussion, it is not a fixed value, its value depends
on what we choose for $a$, and $b$.  Keep in mind that we are trying to find
values for $a$, and $b$ that maximize the likelihood of the linear relationship
between $X$ and $Y$.

So, if $\epsilon_i \sim N\left(0, \sigma^2\right)$, then 

\begin{equation}
ax_i + b + \epsilon_i \sim N\left(ax_i+b, \sigma^2 \right).
\end{equation}

That is, since $\epsilon_i$ is a random variable with mean $0$, then 
the random variable $ax_i+b + \epsilon_i$ will have mean $ax_i+b$.  Orloff
and Bloom show this in \cite{reading6a}.  In this case we are treating 
$ax_i + b$ as constants.  This is really confusing, because we are trying
to find values for $a$ and $b$ that maximize a probability. So we are 
considering varying values of $a$ and $b$ so that we find the best values
for them.  However, assuming we choose values for $a$ and $b$, then 
$ax_i+b+\epsilon_i$ will have mean $ax_i+b$.

In order to make the leap to a probability density function that we are
going to maximize, we cite the reasoning Orloff and Bloom give in 
\cite{reading10b}, section 4.

Then the likelihood function $f_i$ for one point $\left(x_i, y_i\right)$ is:

\begin{equation}
f_i\left(y_i \mid x_i, a, b, \sigma\right)
 = \frac{1}{\sqrt{2\pi}\sigma}
	e^{-\frac{\left(y_i -\left(ax_i+b\right)\right)^2}{2\sigma^2}}.
\end{equation}

The likelihood function $f$ of all points is the product of the function above
for all values of $x_i$, and $y_i$:

\begin{equation}
f
 = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi}\sigma}
	e^{-\frac{\left(y_i -\left(ax_i+b\right)\right)^2}{2\sigma^2}}.
\end{equation}

We can rewrite the product above as:

\begin{equation}
f\left(y_i \mid x_i, a, b, \sigma\right)
 =  \frac{1}{\sqrt{2\pi}\sigma}
	e^{-\frac{\left(\sum_{i=1}^{n} \left(y_i - ax_i+b\right)\right)^2}{2\sigma^2}}.
\end{equation}

The right hand side of the equation above is the likelihood function.

\subsection{Likelihood and log-likelihood functions for particular values}

We suppose we have the following data:

$\left(1,8\right), \left(3,2\right), \left(5,1\right)$.

We write down the liklihood and log likelihood functions for these
data:

\begin{equation}
f\left(y_i \mid x_i, a, b, \sigma\right)
 =  \frac{1}{\sqrt{2\pi}\sigma}
	e^{-
		\frac{
			\left( 8 -\left(a +b\right)\right)^2
			+ \left( 2 -\left(3a +b\right)\right)^2
			+ \left( 1 -\left(5a +b\right)\right)^2
		  }
		{2\sigma^2}
	  }.
\end{equation}


\begin{equation}
ln \left( f\left(y_i \mid x_i, a, b, \sigma\right) \right)
 =  ln \left( \frac{1}{\sqrt{2\pi}\sigma}
	e^{-
		\frac{
			\left( 8 -\left(a +b\right)\right)^2
			+ \left( 2 -\left(3a +b\right)\right)^2
			+ \left( 1 -\left(5a +b\right)\right)^2
		  }
		{2\sigma^2}
	  } \right).
\end{equation}

We simplify the right hand side of the equation above in several steps:

\begin{equation}
ln \left( f\left(y_i \mid x_i, a, b, \sigma\right) \right)
 =  ln \left( \frac{1}{\sqrt{2\pi}\sigma} \right) + 
	ln \left( e^{-
		\frac{
			\left( 8 -\left(a +b\right)\right)^2
			+ \left( 2 -\left(3a +b\right)\right)^2
			+ \left( 1 -\left(5a +b\right)\right)^2
		  }
		{2\sigma^2}
	  } \right).
\end{equation}


\begin{equation}
\begin{aligned}
ln \left( f\left(y_i \mid x_i, a, b, \sigma\right) \right)
 =  ln \left( \frac{1}{\sqrt{2\pi}\sigma} \right) + \\
	\frac{
			-\left( \left( 8 -\left(a +b\right)\right)^2
			+ \left( 2 -\left(3a +b\right)\right)^2
			+ \left( 1 -\left(5a +b\right)\right)^2
		  \right)}
		{2\sigma^2}	 
	 ln \left( e \right).
\end{aligned}
\end{equation}

\begin{equation}
ln \left( f\left(y_i \mid x_i, a, b, \sigma\right) \right)
 =  ln \left( \frac{1}{\sqrt{2\pi}\sigma} \right) + 
		\frac{
			\left(-\left( 8 -\left(a +b\right)\right)^2
			+ \left( 2 -\left(3a +b\right)\right)^2
			+ \left( 1 -\left(5a +b\right)\right)^2\right)
		  }
		{2\sigma^2}.
\end{equation}


\subsubsection{General formulation}

We gave the general formulation for the likelihood function above:

\begin{equation}
f\left(y_i, a, b, \sigma\right)
 =  \frac{1}{\sqrt{2\pi}\sigma}
	e^{-\frac{\left(\sum_{i=1}^{n} \left(y_i -ax_i+b\right)\right)^2}{2\sigma^2}}.
\end{equation}

Note we have removed the $x_i$ from the left hand side of the equation as a function
parameter because the $x_i$ are constants.

We obtain the log likelihood function applying the natrual logarithm function
to both sides of the equation above, and then simplifying using the laws of
logarithms.

\begin{equation}
ln\left(f\left(y_i, a, b, \sigma\right)\right)
 =  ln(left(\frac{1}{\sqrt{2\pi}\sigma}
	e^{-\frac{\left(\sum_{i=1}^{n} \left(y_i -ax_i+b\right)\right)^2}{2\sigma^2}}.
\end{equation}

\begin{equation}
ln\left(f\left(y_i, a, b, \sigma\right)\right)
 =  ln\left(\frac{1}{\sqrt{2\pi}\sigma} \right)
	 + lnleft(e^{-\frac{\left(\sum_{i=1}^{n} \left(y_i - ax_i+b\right)\right)^2}{2\sigma^2}}.
\end{equation}


\begin{equation}
ln\left(f\left(y_i, a, b, \sigma\right)\right)
 =  ln\left(\frac{1}{\sqrt{2\pi}\sigma} \right)
	 + ln\left(e^{-\frac{\left(\sum_{i=1}^{n} \left( y_i - ax_i+b\right)\right)^2}{2\sigma^2}} \right).
\end{equation}

\begin{equation}
ln\left(f\left(y_i, a, b, \sigma\right)\right)
 =  ln\left(\frac{1}{\sqrt{2\pi}\sigma} \right)
    - \frac{\left(\sum_{i=1}^{n} \left( y_i -ax_i+b\right)\right)^2}{2\sigma^2}.
\end{equation}

\subsection{Maximum likelihood estimates for $a$, and $b$}

For this problem, Orloff and Bloom allow us to assume that $\sigma$ is a constant,
known value.  They ask us to find the maximum likelihood estimates for
$a$, and $b$, under these circumstances.

In this case, we will be working with partial derivatives of
\begin{equation}
ln \left( f\left(y_i \mid, a, b, \sigma\right) \right)
 =  ln \left( \frac{1}{\sqrt{2\pi}\sigma} \right) + 
		\frac{
			\left(-\left( 8 -\left(a +b\right)\right)^2
			+ \left( 2 -\left(3a +b\right)\right)^2
			+ \left( 1 -\left(5a +b\right)\right)^2\right)
		  }
		{2\sigma^2},
\end{equation}

At this point we have an exercise in calculus and linear algebra
to obtain two equations in two unknowns by setting the parital 
derivatives of the function $f$ above with respoect to $a$, and $b$
to zero, and then solving the system for $a$ and $b$.

\begin{equation}
\frac{\delta}{\delta a} ln \left( f\left(y_i \mid, a, b, \sigma\right) \right)
 =  	\frac{
			-2\left( 8 -\left(a +b\right)\right)
			- 2\left(6\right) \left( 2 -\left(3a +b\right)\right)
			- 2\left(5\right) \left( 1 -\left(5a +b\right)\right)
		  }
		{2\sigma^2},
\end{equation}

We obtain a similiar partial derivative with respect to $b$, then
solve the resulting system of equations for $a$, and $b$.


\section{Estimating uniform parameters}

subsection{Estimate with set of specific numbers}

Orloff and Bloom give us a dataset $S$:
\begin{equation}
S = \left\{1.2, 2.1, 1.3, 10.5, 5 \right\}
\end{equation}


In \cite{reading10b} Orloff and Bloom state that the likelihood
function for data that follows a uniform distribution over the
interval $\left[a, b \right]$ is maximized when the uniform distribution
has parameters $\hat{a} = \text{min}\left(S\right)$, and $\hat{b} =
\text{max}\left(S\right)$.

Therefore in this example the maximum likelihood estimate for the
distribution that the data in $S$ follows is a uniform distribution
on the interval $\left[ 1.2, 10.5 \right]$.

subsection{Estimate with general set of numbers}

Orloff and Bloom change the question above. They ask us what would be
the maximum likelihood estimate for parameters of a uniform distribution
where we have a data set $S$

\begin{equation}
S=\left{x_1, x_2, \ldots, x_n \right}.
\end{equation}

We just re-defined $S$, here, but the result is the same, the 
maximum likelood estimate of the parameters $a$ and $b$ for the
uniform distribution that the data set is drwan from is
$\hat{a} = \text{min}\left(S\right)$, and $\hat{b} =
\text{max}\left(S\right)$ for $a$, and $b$ respectively.

\section{Monty Hall sober and drunk}
\subsection{Bayes' table}
Hypothesis $A$ is that the car is behind door $A$,
and similarly for hypthotheses $B$, and $C$.

The data, $D$, is that Monty opens door $B$,and reveals a goat.

In order to procede we need to make
some assumptions, but we can make these
assumptions without losing generality.

We are going to assume we chose door $A$.

There are two cases:
\begin{itemize}
\item the car is behind door $A$, and
\item the car is not behind door $A$.
\end{itemize}

If the car is behind door $A$, then there
is a $0.5$ probabilty that hypothesis $A$
is correct because Monty opened door $B$.
Since we chose a door with a car behind
it Monty chose one of the doors with a 
goat behind it with $0.5$ probabilty.  We
capture this case with the first row of the
Bayes table below.


If the car is not behind door $A$, then  we
selected a door with a goat behind it, so
Monty is forced to show us what is behind
the only other door with a goat behind
it, whiich in this case is door $B$. 
Therefore, in this case, the likelihood
of the hypothesis that the car is
behind door $C$ has probabilty 1. We
capture this in the third row of the
Bayes table below.
\begin{center}
\begin{tabular}{ | c | c | c | c  | c | }
    \hline
    hypothesis & prior & likelihood & Bayes numerator & posterior\\ \hline
    $\mathcal{H}$ & $P\left(\mathcal{H}\right)$ & $P\left(D \mid \mathcal{H}\right)$ & $P\left(D \mid \mathcal{H} \right)$ & $P\left(\mathcal{H} \mid D \right)$ \\ \hline
    $A$ & \frac{1}{3} & \frac{1}{2} & \frac{1}{6}  & \frac{1}{3} \\ \hline
    $B$ & \frac{1}{3} & 0           & 0   &  \\ \hline
    $C$ & \frac{1}{3} & 1 & \frac{1}{3}& \frac{2}{3} \\ \hline
    &  &  & \frac{1}{2} &  \\ \hline
  \end{tabular}
\end{center}


\printbibliography{}

\end{document}
