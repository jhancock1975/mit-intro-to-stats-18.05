<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"  
  "http://www.w3.org/TR/html4/loose.dtd">  
<html > 
<head><title>Reading 12a Questions</title> 
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"> 
<meta name="generator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<meta name="originator" content="TeX4ht (http://www.cse.ohio-state.edu/~gurari/TeX4ht/)"> 
<!-- html --> 
<meta name="src" content="reading-12a-questions.tex"> 
<meta name="date" content="2017-07-08 07:48:00"> 
<link rel="stylesheet" type="text/css" href="reading-12a-questions.css"> 
</head><body 
>
   <div class="maketitle">



<h2 class="titleHead">Reading 12a Questions</h2>
<div class="author" ><span 
class="cmr-12">John Hancock</span></div><br />
<div class="date" ><span 
class="cmr-12">July 3, 2017</span></div>
   </div>
   <h3 class="likesectionHead"><a 
 id="x1-1000"></a>Contents</h3>
   <div class="tableofcontents">
   <span class="sectionToc" >1 <a 
href="#x1-20001" id="QQ2-1-2">References and license</a></span>
<br />   <span class="sectionToc" >2 <a 
href="#x1-30002" id="QQ2-1-3">Posterior predictive probability</a></span>
   </div>
   <h3 class="sectionHead"><span class="titlemark">1   </span> <a 
 id="x1-20001"></a>References and license</h3>
<!--l. 15--><p class="noindent" >We are answering questions in the material from MIT OpenCourseWare course 18.05,
Introduction to Probability and Statistics.
<!--l. 18--><p class="indent" >   We use documentation in to write the <span class="LATEX">L<span class="A">A</span><span class="TEX">T<span 
class="E">E</span>X</span></span>code for this document.
<!--l. 21--><p class="indent" >   In this document we are answering questions Orloff and Bloom ask in
<span class="cite">[<a 
href="#Xreading12aqu">1</a>]</span>.
<!--l. 24--><p class="noindent" >
   <h3 class="sectionHead"><span class="titlemark">2   </span> <a 
 id="x1-30002"></a>Posterior predictive probability</h3>
<!--l. 25--><p class="noindent" >In this section we continue the example that Orloff and Bloom give us in <span class="cite">[<a 
href="#Xreading12">2</a>]</span>. They
give us a coin-flipping experiment, where we flip a coin twice, and it lands on heads
twice. The task Orloff and Bloom give us is to work out the posterior predictive
probability of heads on the third toss.
<!--l. 31--><p class="indent" >   We incorporate all the data and probabilities that Orloff and Bloom give us into a
Bayesian update table and add one more column with the numbers Orloff and Bloom
require for the solution to this problem.

<!--l. 35--><p class="indent" >   Note: in the table below, <span 
class="cmmi-10">D</span><sub><span 
class="cmmi-7">p</span></sub> is the probability that the next data we get is heads,
and it is conditioned on the hypothesis that we select a certain type of coin in the
table below.
<div class="center" 
>
<!--l. 39--><p class="noindent" >
<div class="tabular"> <table id="TBL-2" class="tabular" 
cellspacing="0" cellpadding="0" rules="groups" 
><colgroup id="TBL-2-1g"><col 
id="TBL-2-1"></colgroup><colgroup id="TBL-2-2g"><col 
id="TBL-2-2"></colgroup><colgroup id="TBL-2-3g"><col 
id="TBL-2-3"></colgroup><colgroup id="TBL-2-4g"><col 
id="TBL-2-4"></colgroup><colgroup id="TBL-2-5g"><col 
id="TBL-2-5"></colgroup><colgroup id="TBL-2-6g"><col 
id="TBL-2-6"></colgroup><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-1-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-1"  
class="td11">hypothesis</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-2"  
class="td11"> prior </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-3"  
class="td11">  likelihood    </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-4"  
class="td11">Bayes numerator</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-5"  
class="td11"> posterior </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-1-6"  
class="td11"> posterior predictive </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-2-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-1"  
class="td11">         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-2"  
class="td11"> prior </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-3"  
class="td11">flip heads twice</td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-4"  
class="td11">              </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-5"  
class="td11">        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-2-6"  
class="td11">                 </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-3-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-1"  
class="td11">    <span 
class="cmsy-10"><img 
src="cmsy10-48.png" alt="H" class="10x-x-48" />   </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-2"  
class="td11"><span 
class="cmmi-10">P</span><img 
src="reading-12a-questions0x.png" alt="(H )"  class="left" align="middle"></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-3"  
class="td11">  <span 
class="cmmi-10">P</span><img 
src="reading-12a-questions1x.png" alt="(D  | H)"  class="left" align="middle">    </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-4"  
class="td11"><span 
class="cmmi-10">P</span><img 
src="reading-12a-questions2x.png" alt="(D | H)"  class="left" align="middle"><span 
class="cmmi-10">P</span><img 
src="reading-12a-questions3x.png" alt="(H)"  class="left" align="middle"> </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-5"  
class="td11"><span 
class="cmmi-10">P</span><img 
src="reading-12a-questions4x.png" alt="(H | D )"  class="left" align="middle"></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-3-6"  
class="td11"><span 
class="cmmi-10">P</span><img 
src="reading-12a-questions5x.png" alt="(H | D )"  class="left" align="middle"><span 
class="cmmi-10">P</span><img 
src="reading-12a-questions6x.png" alt="(Dp | H)"  class="left" align="middle"></td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-4-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-1"  
class="td11">    <span 
class="cmmi-10">A     </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-2"  
class="td11"> 0.5  </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-3"  
class="td11">     0.25       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-4"  
class="td11">     0.125        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-5"  
class="td11">  0.299   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-4-6"  
class="td11">       0.150          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-5-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-1"  
class="td11">    <span 
class="cmmi-10">B     </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-2"  
class="td11"> 0.25 </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-3"  
class="td11">     0.36       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-4"  
class="td11">     0.09        </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-5"  
class="td11">  0.216   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-5-6"  
class="td11">       0.129          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-6-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-1"  
class="td11">    <span 
class="cmmi-10">C     </span></td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-2"  
class="td11"> 0.25 </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-3"  
class="td11">     0.81       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-4"  
class="td11">    0.2025       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-5"  
class="td11">  0.485   </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-6-6"  
class="td11">       0.437          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-7-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-1"  
class="td11">         </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-2"  
class="td11">     </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-3"  
class="td11">             </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-4"  
class="td11">    0.4125       </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-5"  
class="td11">   1.0    </td><td  style="white-space:nowrap; text-align:center;" id="TBL-2-7-6"  
class="td11">       0.715          </td>
</tr><tr 
class="hline"><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td><td><hr></td></tr><tr  
 style="vertical-align:baseline;" id="TBL-2-8-"><td  style="white-space:nowrap; text-align:center;" id="TBL-2-8-1"  
class="td11">         </td></tr></table></div></div>
<!--l. 52--><p class="indent" >   The weighted sum of the product of the posterior probabilities and the
probability of heads given the type of coin is the posterior predictive probability. In
this case 0<span 
class="cmmi-10">.</span>715.
<!--l. 56--><p class="noindent" >
   <h3 class="likesectionHead"><a 
 id="x1-40002"></a>References</h3>
<!--l. 56--><p class="noindent" >
    <div class="thebibliography">
    <p class="bibitem" ><span class="biblabel">
  [1]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xreading12aqu"></a>Jeremy Orloff and Jonathan Bloom, Reading Questions 12a, Available
    at https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/reading-questions-12a/
    (Spring 2014)
    </p>
    <p class="bibitem" ><span class="biblabel">
  [2]<span class="bibsp">&#x00A0;&#x00A0;&#x00A0;</span></span><a 
 id="Xreading12"></a>Jeremy Orloff and Jonathan Bloom, Bayesian Updating: Probabilistic
    Prediction Class 12, 18.05 Jeremy Orloff and Jonathan Bloom Available at
    https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18&#x02D9;05S14&#x02D9;Reading12a.pdf
    (Spring 2014)
</p>
    </div>
    
</body></html> 



