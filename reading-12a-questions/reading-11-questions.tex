\documentclass{article}

\usepackage{makeidx}
\usepackage{amsmath}
\usepackage{amssymb}

\makeindex
\title{Reading 12a Questions}
\author{John Hancock}
\date{July 3, 2017}
\begin{document}
\maketitle 
\tableofcontents
\section{References and license}
We are answering questions in the material from MIT OpenCourseWare
course 18.05, Introduction to Probability and Statistics.

We use documentation in to write the \LaTeX code for
this document.

In this document we are answering questions Orloff and Bloom ask in
\cite{reading12aqu}.

\section{Posterior predictive probability}
In this section we continue the example that Orloff and Bloom give us in
\cite{reading12}. They give us a coin-flipping experiment, where we flip
a coin twice, and it lands on heads twice.  The task Orloff and Bloom
give us is to work out the posterior predictive probability of heads on
the third toss.

We incorporate all the data and probabilities that Orloff and Bloom give
us into a Bayesian update table and add one more column with the numbers
Orloff and Bloom require for the solution to this problem:

\bbegin{center}
\begin{tabular}{ | c | c | c | c  | c | }
    \hline
    hypothesis & prior & likelihood & Bayes numerator & posterior\\ \hline
    $\mathcal{H}$ & $P\left(\mathcal{H}\right)$ & $P\left(D \mid \mathcal{H}\right)$ & $P\left(D \mid \mathcal{H} \right)$ & $P\left(\mathcal{H} \mid D \right)$ \\ \hline
    $A$ & 0.5  & 0.03125 & 0.015625  & 0.859 \\ \hline
    $B$ & 0.25 & 0.01020 & 0.00256   & 0.141 \\ \hline
    $C$ & 0.25 & 0.00001 & 0.0000025 & 0.000137 \\ \hline
  \end{tabular}
\end{center}

Therefore:

\begin{itemize}
\item $P\left(A \mid D \right) = 0.859$,
\item $P\left(B \mid D \right) = 0.141$,
\item $P\left(C \mid D \right) = 0.000137$,
\end{itemize}


\section{Stubborn priors}
In this problem, Orloff and Bloom give us a scenario with three
different hypotheses; we write the Bayesian update table below
to summarize the scenario, and to calculate the posterior probabilities
that Orloff and Bloom are asking us for.

\begin{center}
\begin{tabular}{ | c | c | c | c  | c | }
    \hline
    hypothesis & prior & likelihood & Bayes numerator & posterior\\ \hline
    $\mathcal{H}$ & $P\left(\mathcal{H}\right)$ & $P\left(D \mid \mathcal{H}\right)$ & $P\left(D \mid \mathcal{H} \right)$ & $P\left(\mathcal{H} \mid D \right)$ \\ \hline
    $A$ & 0.0  & 0.80 & 0.0  & 0.0 \\ \hline
    $B$ & 1    & 0.01 & 0.01 & 1   \\ \hline
    $C$ & 0.0  & 0.7 & 0.0   & 0.0 \\ \hline
  \end{tabular}
\end{center}

Orloff and Bloom ask us if any amount of data will change our belief that
$B$ is the correct hypothesis.  No amount of data will change our belief
that $B$ is the correct hypothesis because the Bayes numerator, and
therefore the posterior probabilities will always be 0 for the hypotheses
$A$, and $C$.

\begin{thebibliography}{99}
\bibitem{reading12aqu}
Jeremy Orloff and Jonathan Bloom,
Reading Questions 12a,
Available at https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/reading-questions-12a/
(Spring 2014)

\bibitem{reading12}
Jeremy Orloff and Jonathan Bloom,
Bayesian Updating: Probabilistic Prediction Class 12, 18.05 Jeremy Orloff and 
Jonathan Bloom
Available at https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading12a.pdf
(Spring 2014)

\end{thebibliography}
\end{document}
