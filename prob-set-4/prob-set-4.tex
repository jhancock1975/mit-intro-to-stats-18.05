\documentclass[a5paper,11pt]{article}


%for coloring cell in a table
\usepackage[table]{xcolor}% http://ctan.org/pkg/xcolor

\usepackage{amsmath}
\usepackage{amssymb}

% for proofs  environment
\usepackage{amsthm}

% for 3d plots
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepgfplotslibrary{patchplots}

\usepackage[backend=bibtex]{biblatex}
\bibliography{prob-set-4}

% for probability trees
\usepackage{tikz}
\usetikzlibrary{trees}

% for Venn diagrams
\usetikzlibrary{shapes,backgrounds}

% for plots
\usepackage{ pgfplots}
% inserted on suggestion in warning during compilation
\pgfplotsset{compat=1.9}

%for strikethrough text
\usepackage{soul}

%for R source code listing
\usepackage{listings}

%for block quotes
\usepackage{csquotes}

\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}

% For not indenting the first line of paragraphs:
\setlength{\parindent}{0pt}
% define the title
\author{John Hancock}
\title{Problem Set 4}
\begin{document}
% generates the title
\maketitle
% insert the table of contents
\tableofcontents
\section{References and License}
We are answering questions in the material from MIT OpenCourseWare
course 18.05, Introduction to Probability and Statistics.

In this document we are answering questions Orloff and Bloom ask in
\cite{slides7}.

Please see the references section for detailed citation information.

The material for the course is licensed under the terms at
\url{http://ocw.mit.edu/terms}.

We use documentation in  \cite{logicNot}, \cite{proofs}, \cite{bars},
\cite{packageClash}, \cite{curlyBrace}, \cite{cases} to write the \LaTeX source code for this document.

\section{Time to failure}
The first group of problems Orloff and Bloom have for us involve some
random variables that follow an exponential distribution.

The exponential distribution they give us to work with has probability
density function (pdf):

\begin{equation}
f\left(x \right) = \lambda e^{-\lambda x}, x \geq 0.
\end{equation}



\subsection{$P\left(X \geq x \right)$}

We know how to calculate $P\left(X < x \right)$ as a definite integral
\cite{reading5b}, therefore we will find 
$P\left( X < x \right)$, and our final result will be to find
$P\left(X \geq x \right) = 1 - P\left( X < x \right)$.

In order to calculate this probability, we will do a change of variable
similar to the technique Orloff and Bloom show in section 3.4 of
\cite{reading7}.

We change the variable in the pdf $f\left(x \right)$ to $u$; therefore we
rewrite the pdf as $f\left( u \right)$:

\begin{equation}
f\left(u \right) = \lambda e^{-\lambda u}.
\end{equation}

We use this definition, the fact that $f$ is defined for $x \geq 0$, and the 
definition of probability of continuous random variables \cite{reading5b} to 
write this equation:

\begin{equation}
P\left(X < x \right) = \int_0^x \lambda e^{-\lambda u} \,du.
\end{equation}

We substitute the integral on the right hand side of the previous equation 
with its antiderivative to get:

\begin{equation}
P\left(X < x \right) = -e^{-\lambda u} \bigg\rvert_{u=0}^x.
\end{equation}

We evaluate the the antiderivative at the limits of integration:

\begin{equation}
P\left(X < x \right) = -e^{-\lambda x} - -e^{-\lambda 0}.
\end{equation}

Now we simplify the previous equation:

\begin{equation}
P\left(X < x \right) = -e^{-\lambda x} + 1.
\end{equation}

Now, we apply the identity:

\begin{equation}
P\left(X \geq x \right) = 1 - P\left(X < \right).
\end{equation}

Therefore
\begin{equation}
P\left(X \geq x \right) = 1 - \left( -e^{-\lambda x} + 1 \right).
\end{equation}

The previous equation simplifies to:

\begin{equation}
P\left(X \geq x \right) = e^{-\lambda x}.
\end{equation}

\subsection{CDF of Minimum of two exponential random variables}

In this section Orloff and Bloom ask us to find the cumulative distribution
function (CDF) of two independent random variables $X_1$, and $X_2$ that both
follow an exponential distribution, and that both have mean 
$\frac{1}{\lambda}$.

In \cite{reading5c} Orloff and Bloom state that the mean of a random
variable that has probability mass function (pmf) $\lambda e^{-\lambda x}$
is $\frac{1}{\lambda}$.

Therefore $X_1$, and $X_2$ both have pmf's $\lambda e^{-\lambda x}$.

For this problem, Orloff and Bloom let $T=\text{min}\left(X_1, X_2 \right)$.

They ask us for the cdf of $T$.

The cdf of $T$ is a function $F\left(t \right) = P \left(T < t \right)$.

In the previous section, we found that for a random varialbe $X$ that has pdf
$\lambda e^{-\lambda x}$, 

\begin{equation}
P\left(X \geq x \right) = e^{-\lambda x}.
\end{equation}

We use the definition of $T$ to write the equation:

\begin{equation}
P\left(T \geq t \right) = 
  P \left(\text{min}\left(X_1, X_2\right) \geq t\right).
\end{equation}


$T=\text{min}\left(X_1, X_2 \right)$, so $T \geq t$ if, and only if, 
$X_1 \geq t$, and $X_2 \geq t$.

\begin{lem}
If two events $A$ and $B$ have a biconditional relation, then

\begin{equation}
P\left(A \right) = P \left( B \right).
\end{equation}

\begin{proof}
The proof is by contradiction.  Assume events $A$ and $B$ are biconditionally
related, but $P\left(A \right) \neq P\left( B \right)$.  Then there would
be unequal chances of events $A$ and $B$ occuring, which means that one event
would occur while the other does not.  But $A$ and $B$ are biconditionally
related, so event $A$ occurs when, and only when, event $B$ occurs. This 
is a contradiction, so $P\left(A \right) = P\left( B \right)$.
\end{proof}
\end{lem}

Therefore
\begin{equation}
P\left(T \geq t \right) = 
  P \left(X_1 \geq t, X_2 \geq t \right).
\end{equation}

$X_1$ and $X_2$ are independent events.  In \cite{reading7} Orloff and Bloom
state that random variables $X_1$ and $Y_1$ are independent if and only if:
\begin{equation}
P\left(X_1, X_2 \right)=F_{X_1}\left(x_1 \right)F_{X_2}\left(x_2\right).
\end{equation}

$F \left(X_1, X_2\right)$ is the cdf of $X_1$, and $X_2$.  
$F_{X_1}\left(x_1 \right)$, and $F_{X_2}\left( x_2 \right)$ are the marginal 
cumulative distribution functions of $X_1$, and $X_2$. 

We know that $X_1$, and $Y_1$ are exponentially distributed random variables 
with mean $\frac{1}{\lambda}$.  The answer we find in the previous section 
implies that the cdf of $X_1$ is $F_{X_1}\left(x_1 \right) = e^{-\lambda x_1}$, and the cdf of $X_2$ 
is $F_{X_2}\left(x_2 \right) = e^{-\lambda x_2}$.

Since $X_1$, and $X_2$ are independent, 

\begin{equation}
F\left(X_1, X_2 \right)= e^{-\lambda x_1} e^{-\lambda x_2}.
\end{equation}

We add the exponents of the base $e$ to simplify the previous equation to:

\begin{equation}
F\left(X_1, X_2 \right)= e^{-\lambda \left(x_1 + x_2\right)}.
\end{equation}

We are finding the cdf of $T$.

Consider $F\left(X_1 < t, X_2 < t \right)$.  We use the previous equation
to write:

\begin{equation}
F\left(X_1 < t, X_2 < t \right)= e^{-\lambda 2t}.
\end{equation}

Since $P\left(T < t \right) = P\left(X_1 < t, X_2 < t \right)$:

\begin{equation}
F\left(T < t, X_2 < t \right)= e^{-\lambda 2t}.
\end{equation}

\subsection{Three lightbulbs}

In this section we answer a question about three lightbulbs, $B1$, $B2$, and
$B3$, where each lighbulb's lifetime is an exponential random variable with
mean values $\frac{1}{2}$, $\frac{1}{3}$, and $\frac{1}{5}$, respectively.
The unit for each mean value is years.

Furthermore, Orloff and Bloom state that the lifetimes of the lightbulbs are
independent.


Let $X_1$, $X_2$, and $X_3$ be the random variables equal to the lifetimes
of $B_1$, $B_2$, and $B_3$, respectively.

Since the mean value of $X_1$ is $\frac{1}{2}$, and $X_1$
follows an exponential distribution,  $X_1 \sim 2e^{-2t}$.

Similarly, $X_2 \sim 3e^{-3t}$, and $X_3 \sim 5e^{-5t}$


We apply logic similar to what we use in the previous question to state
that the cdf of a random variable $T=\text{min}\left(X_1, X_2, X_3 \right)$
is 

\begin{equation}
F\left(T < t \right) = 
	e^{-1 \left(2+ 3 + 5\right) t }
   = e^{-10t}.
\end{equation}

The expected value of a random variable with cdf $e^{10t}$
is $\frac{1}{10}$ year.

\section{Aching Joints}

We deal with the joint distribution of two
continuous random variables $X$, and $Y$.
The probability density function for X, 
and Y is $f\left(x \right) = 
c\left(x^2+xy \right)$. 
Furthermore, $f$ is defined on 
$\left[0,1 \right] \times 
\left[0,1 \right]$.

\subsection{Value of c}
The first thing Orloff and Bloom ask us for
is the value of $c$ in $f$ as defined
above.

In order for $f \left(x, y \right)$ to be a
probabilty distribution function (PDF),
\begin{equation}
c \int_0^1 \int_0^1 x^2 + xy \,dx \,dy
= 1.
\end{equation} 

This is true, if and only if:

\begin{equation}
c \int_0^1 \frac{x^3}{3} + \frac{x^2y}{2} 
\,dy \bigg\rvert_{x=0}^1
= 1.
\end{equation} 

We evaluate the anti-derivative over the
interval indicated in the equation above
to obtain:
 
\begin{equation}
c \int_0^1 \frac{1}{3} + \frac{y}{2} 
\,dy 
= 1.
\end{equation} 

We now replace the integral of the function
of $y$ above with its anti-derivative:

\begin{equation}
c \frac{y}{3} + \frac{y^2}{4} 
\bigg\rvert_{y=0}^1
= 1.
\end{equation} 

And, we now evaluate the anti-derivative
over the interval indicated in the 
equation above:

\begin{equation}
c \frac{1}{3} + \frac{1}{4} 
= 1.
\end{equation} 

Now, we simplify the previos equation:

\begin{equation}
c \frac{7}{12} = 1.
\end{equation} 

And, we solve for $c$:

\begin{equation}
c = \frac{12}{7}.
\end{equation} 

\subsection{Marginal cumulative 
distribution, and probability density 
functions}

Orloff and Bloom are asking us to find four 
functions
\begin{itemize}
\item the marginal CDF $F_{Y} \left( y
  \right)$,
\item the marginal CDF $F_{X} \left( x
  \right)$.
\item the marginal PDF $f_{Y} \left( y
  \right)$, and
\item the marginal PDF $f_{X} \left( x
  \right).
\end{itemize}


\printbibliography{}


\end{document}
