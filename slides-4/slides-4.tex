\documentclass[a4paper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}

% for proofs  environment
\usepackage{amsthm}

\usepackage[backend=bibtex]{biblatex}
\bibliography{slides4}

% for probability trees
\usepackage{tikz}
\usetikzlibrary{trees}

%for strikethrough text
\usepackage{soul}

%for R source code listing
\usepackage{listings}

%for block quotes
\usepackage{csquotes}

% For not indenting the first line of paragraphs:
\setlength{\parindent}{0pt}
% define the title
\author{John Hancock}
\title{MIT Introduction to Statistics 18.05 Slides 4 - Questions }
\begin{document}
% generates the title
\maketitle
% insert the table of contents
\tableofcontents
\section{References and License}
We are answering questions in the material from MIT OpenCourseWare
course 18.05, Introduction to Probability and Statistics.

Please see the references section for detailed citation information.

The material for the course is licensed under the terms at 
\url{http://ocw.mit.edu/terms}.

We are answering the questions that Orloff and Bloom ask in
\cite{slides4}.

We use documentation in \cite{blockQuote} to write \LaTeX source code
for this document.
 
\section{Conditional Probability of Unknown Die}

The first question Orloff and Bloom give in \cite{slides4} is:
\begin{displayquote}

\begin{enumerate}
  \item The Randomizer holds the 6-sided die in one fist and the 8-sided
  die in the other.

  \item The Roller selects one of the Randomizerâ€™s fists and covertly 
   takes the die.

  \item The Roller rolls the die in secret and reports the result to the
  table.
\end{enumerate}

Given the reported number, what is the probability that the 6-sided die
was chosen? 
\end{displayquote}

Note: we needed to see the solution in \cite{slides4Ans} in order to
write the answer to this question.

We have two cases.

The first case is the Roller reports a 7 or an 8. Then the 
probability that the 6-sided die was chosen is 0.

The second case is the Roller reports a number with a value from
1 to six.

We draw a probability tree to get started on a solutionn.  We refer
to the section titled, "Shorthand vs. precise trees," in \cite{reading3}
for guidance on drawing the tree below.

$C$ is the event that the Roller selects the cube shaped $6$-sided die.

$O$ is the event that the Roller selects the octohedron shaped $8$-sided
die.

$R1$, $R2$, \ldots, $R6$ are the events that the Roller reports one, 
two, or so on to six.  

% Set the overall layout of the tree
\tikzstyle{level 1}=[level distance=1cm, sibling distance=5cm]
\tikzstyle{level 2}=[level distance=2cm, sibling distance=1cm]

% Define styles for bags and leafs
\tikzstyle{bag} = [text width=4em, text centered]
\tikzstyle{end} = [circle, minimum width=3pt,fill, inner sep=0pt]

\begin{center}
\begin{tikzpicture}[grow=down, sloped]
\node[end] {}
    child {
        node[bag] {$C$}        
           child {
                node[end, label=right:
                    {$R_{1}$}] {}
                edge from parent
                node[above]  {$\frac{1}{6}$}
            }
            child {
                node[end, label=right:
                    {$R_{2}$}] {}
                edge from parent
                node[above]  {$\frac{1}{6}$}
            }
            child {
                node[end, label=right:
                    {$\ldots$}] {}
                edge from parent
                node[above]  {}
            }
            child {
                node[end, label=right:
                    {$R_{6}$}] {}
                edge from parent
                node[above]  {$\frac{1}{6}$}
            }
            edge from parent 
            node[above]  {$\frac{1}{2}$}
    }
    child {
        node[bag] {$O$}        
        child {
                node[end, label=right:
                    {$R_{1}$}] {}
                edge from parent
                node[above]  {$\frac{1}{8}$}
            }
            child {
                node[end, label=right:
                    {$R_{2}$}] {}
                edge from parent
                node[above]  {$\frac{1}{8}$}
            }
            child {
                node[end, label=right:
                    {$\ldots$}] {}
                edge from parent
                node[above]  {$\frac{1}{8}$}
            }
            child {
                node[end, label=right:
                    {$R_{6}$}] {}
                edge from parent
                node[above]  {$\frac{1}{8}$}
            }
        edge from parent         
            node[above]  {$\frac{1}{2}$}
    };
\end{tikzpicture}
\end{center}

Since the probabilities on all edges in the tree connected to $C$ are
$\frac{1}{6}$, and the probabilities on all edges in the tree connected
to $O$ are $\frac{1}{8}$, we can calculate 
$P \left( C \mid R_{1} \right)$, and the result will be the same for
any of the other leaf nodes in the tree above.  This is because the
calculation will involve the same numbers $\frac{1}{2}$, $\frac{1}{6}$,
and $\frac{1}{8}$, and the same operations on these numbers.

Using the tree above, we can calculate $P \left( R_{1} \mid C \right)$.

Now, we use Bayes' theorem in \cite{reading3} to calculate
$P \left( C \mid R_{1} \right)$

\begin{equation} \label{bayesForCube}
P \left( C \mid R_{1} \right) = 
  \frac{ P \left( R_{1} \mid C \right) P \left( C \right)}
    { P \left( R_{1} \right)}
\end{equation}

Now, we apply definitions for values on various parts of probability
trees using the section titled "Shorthand vs. precise trees," in 
\cite{reading3} to obtain values for the numerator and denominator on
the righthand side of \ref{bayesForCube}.

From the probability tree, 
\begin{equation}
P \left( R_{1} \mid C \right) = 
   \frac{1}{6}
\end{equation}

$P \left( C \right) = \frac{1}{2}$.  Note: we are assuming the Roller
uses either die with equal probability.

We apply Bayes rule \cite{reading3} and the Law of Total Probability
\cite{reading3} to compute $P \left( R_{1} \right)$.

\begin{equation}
  P \left( R_{1} \right)
    = P \left( R_{1} \cap C \right) + P \left( R_{1} \cap O \right)
    = P \left( R_{1} \mid C \right) P \left( C \right)
      + P \left( R_{1} \mid O \right) P \left( O \right)
    = \left( \frac{1}{6} \right)  \left( \frac{1}{2} \right)
      + \left( \frac{1}{8} \right)  \left( \frac{1}{2} \right)
\end{equation}

Now we have values for the numerator and denominator of the right
hand side of \ref{bayesForCube}.

\begin{equation}
P \left( C \mid R_{1} \right) = 
  \frac{ 
    \left( \frac{1}{6} \right) \left( \frac{1}{2} \right) 
    }
    {
      \left( \frac{1}{6} \right) \left( \frac{1}{2} \right) + 
        \left( \frac{1}{8} \right) \left( \frac{1}{2} \right) 
    }
\end{equation}

\section{Concept Questions for CDF and PMF}
In \cite{slides4} Orloff and Bloom give us a table for a random
variable $X$:

\begin{center}
  \begin{tabular}{ | c | c | c | c | c |}
    \hline
    values of $X$ & 1 & 3 & 5 & 7 \\ \hline
    cdf $F \left( a \right)$ & 0.5 & 0.75 & 0.9 & 1 \\ \hline
  \end{tabular}
\end{center}

\subsection{ $P \left( X \leq 3 \right)$ }
$P \left( X \leq 3 \right) = 0.75$. We know this because of the 
definition of cdf, and because Orloff and Bloom give us the
value of the cdf of $X$ for $X \leq 3$.

\subsection{ $P \left( X = 3 \right)$ }

We know 

\begin{equation}
  F \left( X \leq 3 \right) = P \left( X = 3 \right) 
    + P \left( X = 1 \right)
\end{equation}

Therefore

\begin{equation}
  P \left( X = 3 \right)  = F \left( X \leq 3 \right) 
    - P \left( X = 1 \right) 
    = 0.75 - 0.5 = 0.25
\end{equation}

\section{Sum of Binomial Random Variables}

\subsection{Sum of Binomial Random Variables with Same Heads
  Probability}
In \cite{slides4} Orloff an Bloom pose the question that if
$X \sim \text{binomial} \left( n, p \right)$ and 
$Y \sim \text{binomial} \left( m, p \right)$, what distribution
does $X + Y$ follow?

In \cite{reading4} Orloff and Bloom give the definition of a random 
variable that folllows a binomial distribution.

Therefore we apply the definition of a binomial distribution to the
binomial random variables Orloff and Bloom give us in this section:

$X$ is the number of heads in $n$ independent Bernoulli trials, and $Y$
is the number of heads in $m$ independent Bernoulli trials.

For both $X$ and $Y$ we are given that the Bernoulli trials have the
same probablity for success, $p$.

Therefore $X+Y$ is the number of successes in $n+m$ independent
Bernoulli trials with a probability of success $p$.

Therefore 

\begin{equation}
  \left( X + Y \right) \sim \text{binomial} \left( n + m, p \right)
\end{equation}

\subsection{Sum of Binomial Random Variables with Different Heads 
  Probability}

We arrive at the answer by process of elimination. 

In \cite{slides4} Orloff and Bloom give us two random variables: $X$,
and $Z$, where $X \sim \text{binomial} \left( n, p \right)$, and
$Z \sim \text{binomial} \left( n, q \right)$.

Orloff and Bloom then ask us which distribution $X + Z$ follows, and
they give us four options to choose from.  

The first option is $\text{binomial} \left( n, p + q \right)$.  This 
cannot be correct because $X + Z$ is a sum of the number of successes in 
$n + n$ independent Bernoulli trials.

The second option is $text{binomial} \left( n, pq \right)$.  This also
cannot be correct because $X + Z$ is a sum of the number of successes in 
$n + n$ independent Bernoulli trials.

The third option is $\text{binomial} \left( 2n, p + q \right)$.  This
cannot be correct because there is a counterexample.

We construct the counterexample:  suppose 
$X \sim \text{binomial} \left( n, \frac{2}{3} \right)$, and
$Z \sim \text{binomial} \left( n, \frac{2}{3} \right)$.

Then, if the third option were correct, 
$X + Z \sim \text{binomial} \left( 2n, \frac{4}{3} \right)$.  No 
probability can be greater than $1$, so the third option cannot be
correct.

This leaves us with the final option of, ``other.''

\section{Number of Successes Before Second Failure}

In \cite{slides4} Orloff and Bloom ask us to describe the pmf
of a random variable $X$ where $X$ is the number of successes
before the second failure of a sequence of independent Bernoulli trials.

Let $\omega$ be a sequence of trials that fits the description of a
sequence of trials that Orloff and Bloom give in this question.

Let $\Omega$ be the set of all $\omega$.

We assume $\omega$ has $n+2$ trials, where $n$ of the trials are 
successful, and two of the trials are failures.

Orloff and Bloom implicitly state that all the sequences of trials
end in a failure because they are asking for the number of successes
before the second failure.

Therefore the $ \left( n+2 \right) ^ {nd} $ element of $\omega$ is the 
second failure.

We can partition $\Omega$ into $n+1$ disjoint subsets containing one
element each, where each subset has the first failure in a different
position.

Therefore we can apply the Law of Total Probability \cite{reading3} to
comupte the probability of the first failure occuring in any of the 
$n+1$  positions in $\omega$ to be 
$ \left( n + 1 \right) \left( 1 -  p \right)$, where $p$ is the 
probability of a successful Bernoulli trial in $\omega$.

There are $n$ independent successful trials in $\omega$, with
probability $p$, one unsuccessful independent trial in $\omega$ with 
probability $\left ( n + 1 \right) \left( 1 - p \right)$, and one final
unsuccesful independent trial in $\omega$ with probability 
$\left( 1 - p \right)$.

We know from \cite{reading3} that the probability of the union of these
independent events is equal to the product of the probabilities of the 
events.  Therefore 
\begin{equation}
p \left( \omega \right) 
  = p^{n}\left( n + 1 \right) \left( 1 - p \right)^{2}
\end{equation}

\section{Forgetful Geometric Random Variables}

In \cite{slides4} Orloff and Bloom as us to show that for a random
variable $X$ that follows a geometric distribution

\begin{equation}
P \left( X = n + k \mid X \geq n \right) = P \left( X = k \right)
\end{equation}

\begin{proof}

We apply Bayes' theorem \cite{reading3} to get started:

\begin{equation}\label{applyBayes1}
P \left( X = n + k \mid X \geq n \right) = 
  frac
  { 
    P \left( X \geq n \mid X = n + k  \right) P \left( X = n + k \right)
  }
  {
    P \left( X \geq n \right)
  }
\end{equation}

$P \left( X \geq n \mid X = n + k  \right) = 1$.  If we are given that
$X = n + k$, then we arecertain that $X \geq n$.

Therefore we can rewrite equation \ref{applyBayes1}:

\begin{equation}\label{applyBayes1}
P \left( X = n + k \mid X \geq n \right) = 
  frac
  { 
    P \left( X = n + k \right)
  }
  {
    P \left( X \geq n \right)
  }
\end{equation}

\end{proof}
 
\printbibliography{}
\end{document}
