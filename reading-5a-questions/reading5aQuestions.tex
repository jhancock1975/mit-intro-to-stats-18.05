\documentclass[a4paper,11pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}

% for proofs  environment
\usepackage{amsthm}

\usepackage[backend=bibtex]{biblatex}
\bibliography{reading5aQuestions}

% for probability trees
\usepackage{tikz}
\usetikzlibrary{trees}

% for plots
\usepackage{ pgfplots}
% inserted on suggestion in warning during compilation
\pgfplotsset{compat=1.9}

%for strikethrough text
\usepackage{soul}

%for R source code listing
\usepackage{listings}

%for block quotes
\usepackage{csquotes}

% For not indenting the first line of paragraphs:
\setlength{\parindent}{0pt}
% define the title
\author{John Hancock}
\title{MIT Introduction to Statistics 18.05 Reading 5a Questions}
\begin{document}
% generates the title
\maketitle
% insert the table of contents
\tableofcontents
\section{References and License}
We are answering questions in the material from MIT OpenCourseWare
course 18.05, Introduction to Probability and Statistics.

Please see the references section for detailed citation information.

The material for the course is licensed under the terms at
\url{http://ocw.mit.edu/terms}.

We are answering the questions that Orloff and Bloom ask in
\cite{reading5aQuestions}.

\section{Variance of a random variable}
In this section we answer the question that Orloff and Bloom ask regarding
the variance of a discrete random variable.

The following table describes a discrete random variable $X$:

\begin{center}
\begin{tabular}{ | c | c | c | c|}
    \hline
    $X$ & $1$ & $2$ & $4$ \\ \hline
		PMF $p \left(x \right)$ & $0.2$ & $0.3$ & $0.5$ \\ \hline
\end{tabular}
\end{center}

We will use the formula from \cite{reading5a} for variance:
\begin{equation}\label{varFormula}
	Var \left( X \right) = E \left( X^{2} \right) - E \left( X\right)^{2}
\end{equation}

First we compute $E \left( X \right)$:
\begin{equation}
  E \left( X \right) = 1 \times 0.2 + 2 \times 0.3 + 4 \times 0.5
    = 0.2 + 0.6 + 2 = 2.8
\end{equation}

Now we compute $E \left( X^{2} \right)$:
\begin{equation}
  E \left( X^{2} \right)
    = 1 \times 0.2 + 4 \times 0.3 + 16 \times 0.5
    = 0.2 + 1.2 + 8
    = 9.4
\end{equation}

Now we apply the formula for variance in equation \ref{varFormula}:
\begin{equation}
Var \left(X \right)
  = 9.4 - \left( 2.8 \right)^{2}
  = 9.4 - 7.84
  = 1.56
\end{equation}

\section{Apply properties of variance}

The second question in \cite{reading5aQuestions} is in several parts; Orloff
and Bloom ask us several questions where we apply the properties of the variance
of discrete random variables to answer.

Orloff and Bloom write that $X$ is a random variable with mean 2 and variance
3.

\subsection{Variance of constant multiple}
Orloff and Bloom ask is for us to compute $Var \left( 3X \right)$

In \cite{reading5a} Orloff and Bloom show for constants $a$, and $b$
\begin{equation}\label{varMultPlus}
  Var \left( aX + b\right) = a^{2}Var(X)
\end{equation}

Therefore $Var \left( 3X \right) = 9 \times 3 = 27$

\subsection{Variance of multiple plus constant}

Orloff and Bloom pose a second question that is a variation on the previous
question. They ask us to compute $Var \left( 3X + 8 \right)$

However, by \ref{varMultPlus} the answer to this question is no different than
the answer to the previous question.  Therefore, $Var \left( 3X + 8 \right) = 27$

\subsection{Expected value of square}

Orloff and Bloom ask us to compute $E \left( X^{2} \right)$.

We apply equation \ref{varFormula} to compute the answer to this question.

\begin{equation}
	Var \left( X \right) = E \left( X^{2} \right) - E \left( X\right)^{2}
\end{equation}

This implies

\begin{equation}
	3 = E \left( X^{2} \right) - 4
\end{equation}

We rewrite the equation above placing unknown terms on the left hand side,
and known terms on the right hand side of the equation below:

\begin{equation}
	E \left( X^{2} \right) =  4 + 3 = 7
\end{equation}

\section{Further Questions on Same Random Variable}
For this problem, Orloff and Bloom continue to ask us questions on the same
random variable they define in the previous question.

\subsection{Variance of square}
In this section, Orloff and Bloom ask us if we can compute a value for
$Var \left( X^{2} \right)$.

We cannot compute this value because the only information we have about $X$ is
the value of its mean and variance. Terms in the sum to compute
$Var \left( X \right)$,  $ \left( x_{i} - \mu \right)^{2}$
might be very different from terms in the sum to compute
$Var \left( X^{2} \right)$, $\left( x_{i}^{2} - \mu \right)^{2}$.

\subsection{Negative Values}
Orloff and Bloom ask if $X$ can take negative values.

The short answer is yes.  However we can construct an example with the aid
of R.

\begin{lstlisting}
> x = c(-2, -1, 0, 1, 2)
> mean(x)
[1] 0
> var(x)
[1] 2.5
> var(sqrt(3/2.5) * x)
[1] 3
> var(sqrt(3/2.5) * x + 2)
[1] 3
> mean(sqrt(3/2.5) * x + 2)
[1] 2
> sqrt(3/2.5) * x + 2
[1] -0.1908902  0.9045549  2.0000000  3.0954451  4.1908902
>
\end{lstlisting}

To get started, we create a list, and assume the contents of the list are
the product $p \left(x_{i} \right) x_{i}$, of a probability mass function and
the value of a random variable.  Values of a probability mass function are
always positive, so the negative values in the list must be negative values
of the random variable.

We use the properties of variance that Orloff and Bloom show in \cite{reading5a}
to transform the list into a list that has a variance of 3 and a mean of 2.

Hence; the list satisfies the parameters of $X$ given in problem 2 of
\cite{reading5aQuestions}; it has a mean of 2 and a variance of 3.

On the last line of the listing we print out the values of the transformed list.

One sees that the first value in the list is negative.  Since we assumed that
the elements of the list are products of a random variable's value and its
probability mass function's value for that value, the random variable's value
must be negative.  Therefore, in the long way of answering this question, $X$
can have negative values.

\section{Sums of variances and means}
In this section we explore sums of means textem{vs}. sums of variances.

\subsection{Sums of means}
Orloff and Bloom ask whether or not, for any random variables $X$, and $Y$,
$E \left( X + Y \right) = E \left(X \right) + E\left( Y\right)$.

The answer to this question is yes.  In fact, Orloff and Bloom prove this in
\cite{reading4b}.

\subsection{Sums of variances}
As a follow up question, Orloff and Bloom ask if $Var \left( X + Y \right) =
Var \left(X \right) + Var \left( Y \right)$ for any random variables $X$ and
$Y$.  In \cite{reading5a} Orloff and Bloom state that this is true when $X$,
and $Y$ are independent.  Therefore $Var \left( X + Y \right) =
Var \left(X \right) + Var \left( Y \right)$ is not true for any random variables
$X$, and $Y$.

\subsection{Standard deviation of Bernoulli random variable}
Orloff and Bloom ask that if $X \sim \text{Bernoulli}\left( 0.8 \right)$,
what is the standard deviation of $X$.

In \cite{reading5a} Orloff and Bloom show that, if
$X \sim \text{Bernoulli}\left( p \right)$, then
$Var \left( X \right) = \left(1 - p \right) p$.

Also in \cite{reading5a}, Orloff and Bloom state that the definition of the
standard deviation of a random variable is the square root of the variance.

Therefore the standard deviation of $X$ is $\sqrt{\left( 1 - 0.8 \right)0.8}$,
which is the square root of 0.16, which is 0.4.
\printbibliography{}
\end{document}
